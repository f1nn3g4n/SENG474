{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyONk23dn3CF19SATkXYsg3B"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Problem 1. American Handwriting"],"metadata":{"id":"iNvi8uHxvRa0"}},{"cell_type":"markdown","source":["# Part 1: Derivative"],"metadata":{"id":"sxmaAoVsvaa1"}},{"cell_type":"markdown","source":["*f*(x)$_i$ = $\\frac{e_i^x}{\\sum_je_j^x}$ \n","\n","gradient = $\\begin{cases} f(x)_i(1-f(x)_i) & j = i\\\\-f(x)_if(x)_j & j\\ne i \\end{cases}$"],"metadata":{"id":"poFUkX--tItj"}},{"cell_type":"code","source":["import math\n","import numpy as np\n","# help from: https://slowbreathing.github.io/articles/2019-05/softmax-and-its-gradient\n","def softmax(vector):\n","  ''' Takes a vector (np.array) and returns softmax matrix '''\n","  result = []\n","  for row in vector:\n","    vals = np.asarray(row)\n","    result.append(np.exp(vals)/float(sum(np.exp(vals))))\n","\n","  return np.array(result)\n","\n","def softmax_gradient(vector):\n","  ''' \n","  Takes a softmax vector (np.array) and returns the softmax gradient matrix \n","  '''\n","  diag = np.asarray(np.diag(vector))\n","  size = len(diag)\n","  matrix = np.empty((size, size), dtype=float)\n","  for i in range(len(matrix)):\n","    for j in range(len(matrix)):\n","      if i == j:\n","        matrix[i][j] = vector[i] * (1-vector[i])\n","      else:\n","        matrix[i][j] = -vector[i]*vector[j]\n","  return matrix"],"metadata":{"id":"qX8_XeEsyHz3","executionInfo":{"status":"ok","timestamp":1667606198227,"user_tz":420,"elapsed":178,"user":{"displayName":"Finn","userId":"01146107018286362364"}}},"execution_count":86,"outputs":[]},{"cell_type":"code","source":["# Test run of softmax and gradient\n","x = np.array([[1,3,5,7]])\n","sm = softmax(x)\n","print(sm[0])\n","sm_grad = softmax_gradient(sm[0])\n","print()\n","print(sm_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g17LHq6P0S5U","executionInfo":{"status":"ok","timestamp":1667605074631,"user_tz":420,"elapsed":174,"user":{"displayName":"Finn","userId":"01146107018286362364"}},"outputId":"71ec7612-08fa-4eea-8de4-7d7aa05a9b9d"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.00214401 0.0158422  0.11705891 0.86495488]\n","<class 'numpy.float64'>\n","[0.00214401 0.0158422  0.11705891 0.86495488]\n","\n","[[ 2.13941201e-03 -3.39658185e-05 -2.50975338e-04 -1.85447085e-03]\n"," [-3.39658185e-05  1.55912258e-02 -1.85447085e-03 -1.37027892e-02]\n"," [-2.50975338e-04 -1.85447085e-03  1.03356124e-01 -1.01250678e-01]\n"," [-1.85447085e-03 -1.37027892e-02 -1.01250678e-01  1.16807938e-01]]\n"]}]},{"cell_type":"markdown","source":["# Part 2: Simple"],"metadata":{"id":"eRdntJGI0IhP"}},{"cell_type":"markdown","source":["\n","%%time gives the current time, use to determine which is faster\n","Python Engineer on YouTube has good from scratch vids"],"metadata":{"id":"qnfj36Aqx7HD"}},{"cell_type":"markdown","source":["make sure keras input layer dimension should correspond with your scratch program"],"metadata":{"id":"F5ojul3c0Bnm"}},{"cell_type":"markdown","source":["follow up on learning rate for keras in 3. keep it simple"],"metadata":{"id":"_5atbKxn0kKI"}},{"cell_type":"markdown","source":["In later hidden layers, could be smaller, eventually getting into the 10 node output layer"],"metadata":{"id":"7a8ugwpG2idt"}},{"cell_type":"markdown","source":["Below is the code for the neural network structure."],"metadata":{"id":"wB56buv-GhFd"}},{"cell_type":"code","source":["# help from: https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\n","\n","class Layer:\n","  def __init__(self, activ_function, activ_prime_function, in_size, out_size):\n","    '''\n","    Takes as parameters:\n","      activ_function: activation function\n","      activ_prime_function: the gradient of activation function\n","      in_size: input size\n","      out_size: output size\n","    Opting for a more complex layer class than found in link to keep together.\n","    '''\n","    self.input = None\n","    self.input_adjusted = None # used for holding adjusted input\n","    self.output = None\n","    self.activ = activ_function\n","    self.activ_prime = activ_prime_function\n","    self.weights = np.random.rand(in_size, out_size) - 0.5\n","    self.bias = np.random.rand(1, out_size) - 0.5\n","\n","  def forward_prop(self, input_data):\n","    '''\n","    Takes input_data as a parameter, sets self.output as:\n","      activation of dot product of input_data and weights + bias\n","    Returns self.output.\n","    '''\n","    self.input = input_data\n","    self.input_adjusted = np.dot(self.input, self.weights) + self.bias\n","    self.output = self.activ(self.input_adjusted)\n","    return self.output   \n","\n","  def back_prop(self, out_err, learn_rate):\n","    '''\n","    Takes as parameters:\n","      out_err: a given output error\n","      learn_rate: a given learning rate\n","    Returns:\n","      input error from element-wise multiplication of matrices (hidden layer)\n","    '''\n","    adjusted_err = self.activ_prime(self.input_adjusted[0]) * out_err\n","\n","    in_err = np.dot(adjusted_err, self.weights.T)\n","    print(self.weights.shape)\n","    weight_err = np.dot(self.input.T, adjusted_err)\n","    self.weights -= learn_rate * weight_err\n","    self.bias -= learn_rate * adjusted_err\n","    return in_err\n","\n","class NeuralNet:\n","  def __init__(self):\n","    self.layers = []\n","    self.loss = None\n","    self.loss_prime = None\n","    self.err = []\n","  \n","  def add_layer(self, layer):\n","    self.layers.append(layer)\n","\n","  def set_loss(self, loss, loss_prime):\n","    self.loss = loss\n","    self.loss_prime = loss_prime\n","  \n","  def predict(self, input_data):\n","    '''\n","    Returns predicted output for elements in input_data.\n","    '''\n","    result = []\n","    \n","    for element in input_data:\n","      for layer in self.layers:\n","        element = layer.forward_prop(element)\n","      result.append(element)\n","    \n","    return result\n","\n","  def fit(self, x_train, y_train, iterations, learn_rate):\n","    tot_count = len(x_train)\n","    for i in range(iterations):\n","      err = 0\n","      for j in range(tot_count):\n","        # train NN\n","        x = x_train[j]\n","        for layer in self.layers:\n","          x = layer.forward_prop(x)\n","\n","        y = y_train[j]\n","        pred_err = self.loss_prime(y, x)\n","        for layer in reversed(self.layers):\n","          pred_err = layer.back_prop(pred_err, learn_rate)\n","        \n","        # compute loss to store\n","        err += self.loss(y, x)\n","      err /= tot_count\n","      self.err.append(err)"],"metadata":{"id":"m05YDwTm2hr_","executionInfo":{"status":"ok","timestamp":1667607014092,"user_tz":420,"elapsed":174,"user":{"displayName":"Finn","userId":"01146107018286362364"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["# sigmoid functions\n","def sigmoid(x):\n","  return 1/(1+np.exp(-x))\n","\n","def sigmoid_prime(x):\n","  return sigmoid(x) * (1 - sigmoid(x))\n","\n","# L2 error functions\n","def mse(x, x_pred):\n","  return np.mean((np.power(x-x_pred, 2)))\n","\n","def mse_prime(x, x_pred):\n","  return 2*(x_pred-x)/x.size;"],"metadata":{"id":"IOUSPQE7LXP3","executionInfo":{"status":"ok","timestamp":1667603014438,"user_tz":420,"elapsed":183,"user":{"displayName":"Finn","userId":"01146107018286362364"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["# Test run of NN for debugging and validation\n","def tanh(x):\n","    return np.tanh(x);\n","\n","def tanh_prime(x):\n","    return 1-np.tanh(x)**2;\n","\n","x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]]])\n","y_train = np.array([[[0]], [[1]], [[1]], [[0]]])\n","\n","nn = NeuralNet()\n","nn.add_layer(Layer(tanh, tanh_prime, 2, 3))\n","nn.add_layer(Layer(tanh, tanh_prime, 3, 1))\n","\n","nn.set_loss(mse, mse_prime)\n","nn.fit(x_train, y_train, 1000, 0.1)\n","\n","print(nn.err[-1])"],"metadata":{"id":"sxOfLKd8dx5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.datasets import mnist\n","from keras.utils import np_utils\n","\n","(train_X, train_y), (test_X, test_y) = mnist.load_data()"],"metadata":{"id":"qx1rADq4ileH","executionInfo":{"status":"ok","timestamp":1667602843916,"user_tz":420,"elapsed":404,"user":{"displayName":"Finn","userId":"01146107018286362364"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# reshape and normalize input data, copied from earlier mentioned article\n","train_X = train_X.reshape(train_X.shape[0], 1, 28*28)\n","train_X = train_X.astype('float32')\n","train_X /= 255\n","# encode output which is a number in range [0,9] into a vector of size 10\n","# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","train_y = np_utils.to_categorical(train_y)\n","\n","# same for test data : 10000 samples\n","test_X = test_X.reshape(test_X.shape[0], 1, 28*28)\n","test_X = test_X.astype('float32')\n","test_X /= 255\n","test_y = np_utils.to_categorical(test_y)\n","\n","# end of copied normalization"],"metadata":{"id":"dibQ52MN3_gS","executionInfo":{"status":"ok","timestamp":1667602846136,"user_tz":420,"elapsed":159,"user":{"displayName":"Finn","userId":"01146107018286362364"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["nn_mnist = NeuralNet()\n","input_layer = Layer(sigmoid, sigmoid_prime, 28*28, 128)\n","output_layer = Layer(softmax, softmax_gradient, 128, 10)\n","nn_mnist.add_layer(input_layer)\n","nn_mnist.add_layer(output_layer)\n","nn_mnist.set_loss(mse, mse_prime)\n","nn_mnist.fit(train_X, train_y, 10, 0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"Ga_SQDC9gjyr","executionInfo":{"status":"error","timestamp":1667607017334,"user_tz":420,"elapsed":179,"user":{"displayName":"Finn","userId":"01146107018286362364"}},"outputId":"cfb48e1c-ddb7-4fed-ffa0-ad7febad5f3a"},"execution_count":91,"outputs":[{"output_type":"stream","name":"stdout","text":["(128, 10)\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-91-993eabc0d3a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnn_mnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnn_mnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_prime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnn_mnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-90-af5313b05b3d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, iterations, learn_rate)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mpred_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_prime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m           \u001b[0mpred_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# compute loss to store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-90-af5313b05b3d>\u001b[0m in \u001b[0;36mback_prop\u001b[0;34m(self, out_err, learn_rate)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0min_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjusted_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mweight_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madjusted_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: shapes (128,1) and (10,10) not aligned: 1 (dim 1) != 10 (dim 0)"]}]},{"cell_type":"code","source":["temp = np.array([[2.11068126,  1.47989292,  3.84471379,  1.01363238, -2.13620242,  1.11731586,\n","  -2.33209269,  3.35144424, -1.04378665, -0.36827289]])\n","check = temp[0]*1-temp[0]\n","print(type(check))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"knGIxy3_-iXO","executionInfo":{"status":"ok","timestamp":1667605228241,"user_tz":420,"elapsed":214,"user":{"displayName":"Finn","userId":"01146107018286362364"}},"outputId":"f05739f7-8156-4797-f8b0-b9bf32d81365"},"execution_count":81,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n"]}]}]}